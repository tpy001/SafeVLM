
batch_size: 16
seed: 0


harmful_dataset:
    _target_: src.dataset.SPA_VL
    data_path: data/SPA_VL/
    split: train
    # split: test
    question: question
    answer: chosen
    image: image_name
    # image: image
    safe: false
    # sample: 32
    seed: 0


normal_dataset:
    _target_: src.dataset.VLSafe
    # data_path: data/VLSafe/VLSafe
    data_path: data/VLSafe_normal
    data_files: helpful_sampled.jsonl  # test dataset
    # data_files: harmlessnss_alignment.jsonl  # train dataset
    split: train
    question: query
    answer: reference
    image: image_id
    COCO_img_path: data/VLSafe/images
    safe: True
    # sample: 300
    seed: 0


model:
    _target_:  src.model.PromptTuningLlava
    pretrained_path: work_dirs/prompt_embedding_iter_1.pth
    vlm_model: 
        _target_:  src.model.LlaVA
        model_path: /data3/tangpeiyuan/LLM/weights/llava-1.5-7B/
        torch_dtype: float16
        device: auto
        trainable: frozen
        assistant_tag: "ASSISTANT:"
        generate_config:
            type: GenerationConfig
            return_full_text: false
            use_cache: true
            bos_token_id: 1
            do_sample: false
            eos_token_id: 2
            pad_token_id: 32001
            # temperature: 0.6
            # top_p: 0.9
            max_new_tokens: 256

    peft_config:
        num: 4
        dim: 4096
        prompt_text: <soft_prompt>
        init_text: If the input is unsafe, refuse to answer. Otherwise, respond helpfully


evaluator:
    _target_: src.evaluator.LLamaGuard3
    model_path: /data3/tangpeiyuan/LLM/weights/Llama3-Guard-8b/


training_set: 
    _target_: src.dataset.SafetyAlignedDataset
    harmful_data: ${harmful_dataset}
    normal_data: ${normal_dataset}

val_dataset:  
    _target_: src.dataset.SPA_VL
    data_path: data/SPA_VL/
    split: test
    question: question
    answer: chosen
    image_PATH: image_name
    image: image
    safe: false
    sample: 128 # sample 128 data
    seed: 0

     
train_config:
      train_batchsize: 4
      val_batchsize: ${batch_size}
      seed: ${seed}
      epoch: 20
      val_interval: 250
      lr: 0.00001
      save_interval: 500
      save_path: ./work_dirs/
      optimizer: 
          type: AdamW
          lr: 0.0001
          betas: [0.9, 0.999]
          eps: 1e-08
          weight_decay: 0.01
